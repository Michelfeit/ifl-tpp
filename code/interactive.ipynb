{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import dpp\n",
    "import numpy as np\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "dataset_name = 'synth/hawkes1'  # other: ['stack_overflow', 'lastfm',\n",
    "#          'synth/poisson', 'synth/renewal', 'synth/self_correcting',\n",
    "#          'synth/hawkes1', 'synth/hawkes2']\n",
    "\n",
    "# Model config\n",
    "context_size = 64                 # Size of the RNN hidden vector\n",
    "mark_embedding_size = 32          # Size of the mark embedding (used as RNN input)\n",
    "num_mix_components = 64           # Number of components for a mixture model\n",
    "rnn_type = \"GRU\"                  # What RNN to use as an encoder {\"RNN\", \"GRU\", \"LSTM\"}\n",
    "\n",
    "# Training config\n",
    "batch_size = 64        # Number of sequences in a batch\n",
    "regularization = 1e-5  # L2 regularization parameter\n",
    "learning_rate = 3e-4   # Learning rate for Adam optimizer\n",
    "max_epochs = 1000      # For how many epochs to train\n",
    "display_step = 5       # Display training statistics after every display_step\n",
    "patience = 50          # After how many consecutive epochs without improvement of val loss to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "dataset = dpp.data.load_dataset(dataset_name)\n",
    "d_train, d_val, d_test = dataset.train_val_test_split(seed=seed)\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=True)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model = dpp.models.LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "opt = torch.optim.Adam(model.parameters(), weight_decay=regularization, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_loss_over_dataloader(dl):\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            total_loss += -model.log_prob(batch).sum().item()\n",
    "            total_count += batch.size\n",
    "    return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch    0: loss_train_last_batch = 69.4, loss_val = 62.6\n",
      "Epoch    5: loss_train_last_batch = 50.3, loss_val = 53.5\n",
      "Epoch   10: loss_train_last_batch = 31.5, loss_val = 30.8\n",
      "Epoch   15: loss_train_last_batch = -7.3, loss_val = -20.0\n",
      "Epoch   20: loss_train_last_batch = -245.4, loss_val = -230.9\n",
      "Epoch   25: loss_train_last_batch = -324.5, loss_val = -328.3\n",
      "Epoch   30: loss_train_last_batch = -346.1, loss_val = -391.1\n",
      "Epoch   35: loss_train_last_batch = -407.1, loss_val = -396.0\n",
      "Epoch   40: loss_train_last_batch = -359.3, loss_val = -404.1\n",
      "Epoch   45: loss_train_last_batch = -429.6, loss_val = -407.1\n",
      "Epoch   50: loss_train_last_batch = -458.1, loss_val = -413.2\n",
      "Epoch   55: loss_train_last_batch = -375.5, loss_val = -414.2\n",
      "Epoch   60: loss_train_last_batch = -318.2, loss_val = -419.7\n",
      "Epoch   65: loss_train_last_batch = -362.3, loss_val = -420.1\n",
      "Epoch   70: loss_train_last_batch = -509.6, loss_val = -420.7\n",
      "Epoch   75: loss_train_last_batch = -352.5, loss_val = -424.6\n",
      "Epoch   80: loss_train_last_batch = -539.6, loss_val = -423.9\n",
      "Epoch   85: loss_train_last_batch = -490.2, loss_val = -425.9\n",
      "Epoch   90: loss_train_last_batch = -381.9, loss_val = -422.5\n",
      "Epoch   95: loss_train_last_batch = -334.7, loss_val = -424.3\n",
      "Epoch  100: loss_train_last_batch = -419.1, loss_val = -415.0\n",
      "Epoch  105: loss_train_last_batch = -442.3, loss_val = -426.3\n",
      "Epoch  110: loss_train_last_batch = -353.2, loss_val = -425.1\n",
      "Epoch  115: loss_train_last_batch = -458.2, loss_val = -424.8\n",
      "Epoch  120: loss_train_last_batch = -503.6, loss_val = -424.6\n",
      "Epoch  125: loss_train_last_batch = -464.8, loss_val = -422.1\n",
      "Epoch  130: loss_train_last_batch = -441.0, loss_val = -424.1\n",
      "Epoch  135: loss_train_last_batch = -484.0, loss_val = -421.5\n",
      "Epoch  140: loss_train_last_batch = -402.6, loss_val = -422.5\n",
      "Epoch  145: loss_train_last_batch = -422.3, loss_val = -412.2\n",
      "Epoch  150: loss_train_last_batch = -455.8, loss_val = -412.7\n",
      "Epoch  155: loss_train_last_batch = -206.0, loss_val = -338.6\n",
      "Breaking due to early stopping at epoch 157\n"
     ]
    }
   ],
   "source": [
    "# Traning\n",
    "print('Starting training...')\n",
    "\n",
    "impatient = 0\n",
    "best_loss = np.inf\n",
    "best_model = deepcopy(model.state_dict())\n",
    "training_val_losses = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    for batch in dl_train:\n",
    "        opt.zero_grad()\n",
    "        loss = -model.log_prob(batch).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_val = aggregate_loss_over_dataloader(dl_val)\n",
    "        training_val_losses.append(loss_val)\n",
    "\n",
    "    if (best_loss - loss_val) < 1e-4:\n",
    "        impatient += 1\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    else:\n",
    "        best_loss = loss_val\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        impatient = 0\n",
    "\n",
    "    if impatient >= patience:\n",
    "        print(f'Breaking due to early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "    if epoch % display_step == 0:\n",
    "        print(f\"Epoch {epoch:4d}: loss_train_last_batch = {loss.item():.1f}, loss_val = {loss_val:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# All training & testing sequences stacked into a single batch\n",
    "with torch.no_grad():\n",
    "    final_loss_train = aggregate_loss_over_dataloader(dl_train)\n",
    "    final_loss_val = aggregate_loss_over_dataloader(dl_val)\n",
    "    final_loss_test = aggregate_loss_over_dataloader(dl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative log-likelihood:\n",
      " - Train: -426.2\n",
      " - Val:   -426.4\n",
      " - Test:  -446.9\n"
     ]
    }
   ],
   "source": [
    "print(f'Negative log-likelihood:\\n'\n",
    "      f' - Train: {final_loss_train:.1f}\\n'\n",
    "      f' - Val:   {final_loss_val:.1f}\\n'\n",
    "      f' - Test:  {final_loss_test:.1f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
